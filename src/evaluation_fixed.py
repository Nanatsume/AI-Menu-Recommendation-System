"""
Enhanced Evaluation Module for AI Menu Recommendation System
‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏°‡∏ô‡∏π‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ font ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
plt.rcParams['font.family'] = ['DejaVu Sans', 'Tahoma']

class AdvancedRecommendationEvaluator:
    """‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô"""
    
    def __init__(self, recommendation_model=None):
        """
        Initialize the evaluator
        
        Parameters:
        recommendation_model: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        """
        self.model = recommendation_model
        self.evaluation_results = {}
        
    def precision_at_k(self, recommended_items, relevant_items, k_values=[5, 10, 20]):
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Precision@K ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö k ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤
        
        Precision@K = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô K ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å / K
        """
        results = {}
        for k in k_values:
            if k == 0 or len(recommended_items) == 0:
                results[f'precision@{k}'] = 0.0
                continue
                
            recommended_k = recommended_items[:k]
            relevant_recommended = set(recommended_k) & set(relevant_items)
            results[f'precision@{k}'] = len(relevant_recommended) / min(k, len(recommended_items))
        
        return results
    
    def recall_at_k(self, recommended_items, relevant_items, k_values=[5, 10, 20]):
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Recall@K ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö k ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤
        
        Recall@K = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô K ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å / ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        """
        results = {}
        for k in k_values:
            if len(relevant_items) == 0:
                results[f'recall@{k}'] = 0.0
                continue
                
            recommended_k = recommended_items[:k]
            relevant_recommended = set(recommended_k) & set(relevant_items)
            results[f'recall@{k}'] = len(relevant_recommended) / len(relevant_items)
        
        return results
    
    def f1_at_k(self, recommended_items, relevant_items, k_values=[5, 10, 20]):
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì F1-Score@K ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô harmonic mean ‡∏Ç‡∏≠‡∏á Precision ‡πÅ‡∏•‡∏∞ Recall"""
        precision_results = self.precision_at_k(recommended_items, relevant_items, k_values)
        recall_results = self.recall_at_k(recommended_items, relevant_items, k_values)
        
        f1_results = {}
        for k in k_values:
            p = precision_results[f'precision@{k}']
            r = recall_results[f'recall@{k}']
            
            if p + r == 0:
                f1_results[f'f1@{k}'] = 0.0
            else:
                f1_results[f'f1@{k}'] = 2 * (p * r) / (p + r)
        
        return f1_results
    
    def hit_rate_at_k(self, recommended_items, relevant_items, k_values=[5, 10, 20]):
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Hit Rate@K 
        
        Hit Rate@K = 1 ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô K ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å, 0 ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
        """
        results = {}
        for k in k_values:
            recommended_k = recommended_items[:k]
            hit = 1.0 if len(set(recommended_k) & set(relevant_items)) > 0 else 0.0
            results[f'hit_rate@{k}'] = hit
        
        return results
    
    def mean_reciprocal_rank(self, recommended_items, relevant_items):
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Mean Reciprocal Rank (MRR)
        
        MRR = 1 / ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
        """
        for i, item in enumerate(recommended_items):
            if item in relevant_items:
                return 1.0 / (i + 1)
        return 0.0
    
    def ndcg_at_k(self, recommended_items, relevant_items, k_values=[5, 10, 20]):
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Normalized Discounted Cumulative Gain (NDCG@K)
        
        NDCG ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
        """
        results = {}
        
        for k in k_values:
            # DCG calculation
            dcg = 0.0
            for i, item in enumerate(recommended_items[:k]):
                if item in relevant_items:
                    dcg += 1.0 / np.log2(i + 2)  # +2 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ log2(1) = 0
            
            # IDCG calculation (ideal DCG)
            idcg = 0.0
            for i in range(min(k, len(relevant_items))):
                idcg += 1.0 / np.log2(i + 2)
            
            # NDCG
            if idcg == 0:
                results[f'ndcg@{k}'] = 0.0
            else:
                results[f'ndcg@{k}'] = dcg / idcg
        
        return results
    
    def catalog_coverage(self, all_recommendations, total_items):
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Catalog Coverage - ‡∏£‡πâ‡∏≠‡∏¢‡∏•‡∏∞‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
        
        Coverage = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏°‡∏ô‡∏π‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ / ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        """
        unique_recommended = set()
        for recs in all_recommendations:
            unique_recommended.update(recs)
        
        return len(unique_recommended) / total_items
    
    def diversity_score(self, recommendations, item_features):
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Diversity Score - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
        
        ‡∏ß‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡πÄ‡∏°‡∏ô‡∏π‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
        """
        if len(recommendations) < 2:
            return 0.0
        
        total_pairs = 0
        diverse_pairs = 0
        
        for i in range(len(recommendations)):
            for j in range(i + 1, len(recommendations)):
                total_pairs += 1
                
                # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡πÄ‡∏°‡∏ô‡∏π
                item1_category = item_features.get(recommendations[i], {}).get('category', '')
                item2_category = item_features.get(recommendations[j], {}).get('category', '')
                
                if item1_category != item2_category:
                    diverse_pairs += 1
        
        return diverse_pairs / total_pairs if total_pairs > 0 else 0.0
    
    def novelty_score(self, recommendations, item_popularity):
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Novelty Score - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏Å‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
        
        ‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡∏°‡∏µ‡∏Ñ‡∏ô‡∏™‡∏±‡πà‡∏á‡∏à‡∏∞‡∏°‡∏µ novelty ‡∏™‡∏π‡∏á
        """
        if len(recommendations) == 0:
            return 0.0
        
        novelty = 0.0
        for item in recommendations:
            # novelty = -log2(popularity)
            popularity = item_popularity.get(item, 0.1)  # default ‡∏ï‡πà‡∏≥‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            novelty += -np.log2(max(popularity, 0.001))  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô log(0)
        
        return novelty / len(recommendations)
    
    def evaluate_comprehensive(self, user_item_matrix, df_menu, df_orders, test_ratio=0.2):
        """
        ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
        
        Parameters:
        user_item_matrix: DataFrame ‡∏Ç‡∏≠‡∏á user-item interaction
        df_menu: DataFrame ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏°‡∏ô‡∏π
        df_orders: DataFrame ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠
        test_ratio: ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö
        """
        print("üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô...")
        
        if self.model is None:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô")
            return None
        
        # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô train/test
        test_users = user_item_matrix.sample(frac=test_ratio).index
        
        all_metrics = {
            'precision': defaultdict(list),
            'recall': defaultdict(list), 
            'f1': defaultdict(list),
            'hit_rate': defaultdict(list),
            'ndcg': defaultdict(list),
            'mrr': [],
            'diversity': [],
            'novelty': []
        }
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì item popularity ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö novelty
        item_popularity = df_orders.groupby('menu_id').size() / len(df_orders)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á item features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö diversity
        item_features = df_menu.set_index('menu_id')[['category']].to_dict('index')
        
        all_recommendations = []
        
        print(f"üìä ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {len(test_users)} ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ...")
        
        for user_id in test_users:
            try:
                # ‡∏´‡∏≤ user index ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
                user_idx = list(user_item_matrix.index).index(user_id)
                
                # ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
                recommendations = self.model.predict_for_user(user_idx, top_k=20)
                
                # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á item IDs
                recommended_items = [item[0] if isinstance(item, tuple) else item 
                                   for item in recommendations]
                
                # ‡∏´‡∏≤‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏™‡∏±‡πà‡∏á (relevant items)
                relevant_items = user_item_matrix.loc[user_id]
                relevant_items = relevant_items[relevant_items > 0].index.tolist()
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏ï‡πà‡∏≤‡∏á‡πÜ
                precision_results = self.precision_at_k(recommended_items, relevant_items)
                recall_results = self.recall_at_k(recommended_items, relevant_items)
                f1_results = self.f1_at_k(recommended_items, relevant_items)
                hit_rate_results = self.hit_rate_at_k(recommended_items, relevant_items)
                ndcg_results = self.ndcg_at_k(recommended_items, relevant_items)
                
                # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                for metric, values in precision_results.items():
                    all_metrics['precision'][metric].append(values)
                for metric, values in recall_results.items():
                    all_metrics['recall'][metric].append(values)
                for metric, values in f1_results.items():
                    all_metrics['f1'][metric].append(values)
                for metric, values in hit_rate_results.items():
                    all_metrics['hit_rate'][metric].append(values)
                for metric, values in ndcg_results.items():
                    all_metrics['ndcg'][metric].append(values)
                
                all_metrics['mrr'].append(self.mean_reciprocal_rank(recommended_items, relevant_items))
                all_metrics['diversity'].append(self.diversity_score(recommended_items[:10], item_features))
                all_metrics['novelty'].append(self.novelty_score(recommended_items[:10], item_popularity))
                
                all_recommendations.append(recommended_items)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error evaluating user {user_id}: {e}")
                continue
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á metrics ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        final_results = {
            'precision@5': np.mean(all_metrics['precision']['precision@5']),
            'precision@10': np.mean(all_metrics['precision']['precision@10']),
            'precision@20': np.mean(all_metrics['precision']['precision@20']),
            'recall@5': np.mean(all_metrics['recall']['recall@5']),
            'recall@10': np.mean(all_metrics['recall']['recall@10']),
            'recall@20': np.mean(all_metrics['recall']['recall@20']),
            'f1@5': np.mean(all_metrics['f1']['f1@5']),
            'f1@10': np.mean(all_metrics['f1']['f1@10']),
            'f1@20': np.mean(all_metrics['f1']['f1@20']),
            'hit_rate@5': np.mean(all_metrics['hit_rate']['hit_rate@5']),
            'hit_rate@10': np.mean(all_metrics['hit_rate']['hit_rate@10']),
            'hit_rate@20': np.mean(all_metrics['hit_rate']['hit_rate@20']),
            'ndcg@5': np.mean(all_metrics['ndcg']['ndcg@5']),
            'ndcg@10': np.mean(all_metrics['ndcg']['ndcg@10']),
            'ndcg@20': np.mean(all_metrics['ndcg']['ndcg@20']),
            'mrr': np.mean(all_metrics['mrr']),
            'diversity': np.mean(all_metrics['diversity']),
            'novelty': np.mean(all_metrics['novelty']),
            'catalog_coverage': self.catalog_coverage(all_recommendations, len(df_menu))
        }
        
        self.evaluation_results = final_results
        
        print("‚úÖ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
        return final_results
    
    def print_evaluation_results(self):
        """‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"""
        if not self.evaluation_results:
            print("‚ùå ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô evaluate_comprehensive ‡∏Å‡πà‡∏≠‡∏ô")
            return
        
        print("\n" + "="*50)
        print("üìä ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏°‡∏ô‡∏π‡∏≠‡∏≤‡∏´‡∏≤‡∏£")
        print("="*50)
        
        print("\nüéØ Accuracy Metrics:")
        print(f"   Precision@5:  {self.evaluation_results['precision@5']:.4f}")
        print(f"   Precision@10: {self.evaluation_results['precision@10']:.4f}")
        print(f"   Precision@20: {self.evaluation_results['precision@20']:.4f}")
        
        print(f"\n   Recall@5:     {self.evaluation_results['recall@5']:.4f}")
        print(f"   Recall@10:    {self.evaluation_results['recall@10']:.4f}")
        print(f"   Recall@20:    {self.evaluation_results['recall@20']:.4f}")
        
        print(f"\n   F1-Score@5:   {self.evaluation_results['f1@5']:.4f}")
        print(f"   F1-Score@10:  {self.evaluation_results['f1@10']:.4f}")
        print(f"   F1-Score@20:  {self.evaluation_results['f1@20']:.4f}")
        
        print("\nüé™ Ranking Metrics:")
        print(f"   Hit Rate@5:   {self.evaluation_results['hit_rate@5']:.4f}")
        print(f"   Hit Rate@10:  {self.evaluation_results['hit_rate@10']:.4f}")
        print(f"   Hit Rate@20:  {self.evaluation_results['hit_rate@20']:.4f}")
        
        print(f"\n   NDCG@5:       {self.evaluation_results['ndcg@5']:.4f}")
        print(f"   NDCG@10:      {self.evaluation_results['ndcg@10']:.4f}")
        print(f"   NDCG@20:      {self.evaluation_results['ndcg@20']:.4f}")
        
        print(f"\n   MRR:          {self.evaluation_results['mrr']:.4f}")
        
        print("\nüåà Beyond Accuracy:")
        print(f"   Diversity:    {self.evaluation_results['diversity']:.4f}")
        print(f"   Novelty:      {self.evaluation_results['novelty']:.4f}")
        print(f"   Coverage:     {self.evaluation_results['catalog_coverage']:.4f}")
        
        print("\nüìù Interpretation Guide:")
        print("   ‚Ä¢ Precision: ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        print("   ‚Ä¢ Recall: ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥")
        print("   ‚Ä¢ F1-Score: ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Æ‡∏≤‡∏£‡πå‡πÇ‡∏°‡∏ô‡∏¥‡∏Å‡∏Ç‡∏≠‡∏á Precision ‡πÅ‡∏•‡∏∞ Recall")
        print("   ‚Ä¢ Hit Rate: ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        print("   ‚Ä¢ NDCG: ‡∏Ñ‡∏≥‡∏ô‡∏∂‡∏á‡∏ñ‡∏∂‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        print("   ‚Ä¢ MRR: ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á reciprocal rank ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        print("   ‚Ä¢ Diversity: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡πÄ‡∏°‡∏ô‡∏π")
        print("   ‚Ä¢ Novelty: ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏Å‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥")
        print("   ‚Ä¢ Coverage: ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏°‡∏ô‡∏π‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥")
    
    def plot_evaluation_metrics(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"""
        if not self.evaluation_results:
            print("‚ùå ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Precision, Recall, F1 @K
        k_values = [5, 10, 20]
        precision_values = [self.evaluation_results[f'precision@{k}'] for k in k_values]
        recall_values = [self.evaluation_results[f'recall@{k}'] for k in k_values]
        f1_values = [self.evaluation_results[f'f1@{k}'] for k in k_values]
        
        axes[0, 0].plot(k_values, precision_values, 'bo-', label='Precision@K', linewidth=2)
        axes[0, 0].plot(k_values, recall_values, 'ro-', label='Recall@K', linewidth=2)
        axes[0, 0].plot(k_values, f1_values, 'go-', label='F1-Score@K', linewidth=2)
        axes[0, 0].set_xlabel('K')
        axes[0, 0].set_ylabel('Score')
        axes[0, 0].set_title('üìà Precision, Recall, F1-Score @K', fontweight='bold')
        axes[0, 0].legend()
        axes[0, 0].grid(alpha=0.3)
        
        # 2. Hit Rate ‡πÅ‡∏•‡∏∞ NDCG @K
        hit_rate_values = [self.evaluation_results[f'hit_rate@{k}'] for k in k_values]
        ndcg_values = [self.evaluation_results[f'ndcg@{k}'] for k in k_values]
        
        axes[0, 1].plot(k_values, hit_rate_values, 'mo-', label='Hit Rate@K', linewidth=2)
        axes[0, 1].plot(k_values, ndcg_values, 'co-', label='NDCG@K', linewidth=2)
        axes[0, 1].set_xlabel('K')
        axes[0, 1].set_ylabel('Score')
        axes[0, 1].set_title('üéØ Hit Rate ‡πÅ‡∏•‡∏∞ NDCG @K', fontweight='bold')
        axes[0, 1].legend()
        axes[0, 1].grid(alpha=0.3)
        
        # 3. Beyond Accuracy Metrics
        beyond_metrics = ['MRR', 'Diversity', 'Novelty', 'Coverage']
        beyond_values = [
            self.evaluation_results['mrr'],
            self.evaluation_results['diversity'],
            self.evaluation_results['novelty'],
            self.evaluation_results['catalog_coverage']
        ]
        
        bars = axes[1, 0].bar(beyond_metrics, beyond_values, 
                             color=['skyblue', 'lightgreen', 'orange', 'pink'])
        axes[1, 0].set_ylabel('Score')
        axes[1, 0].set_title('üåà Beyond Accuracy Metrics', fontweight='bold')
        axes[1, 0].set_ylim(0, 1)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏ö‡∏ô‡πÅ‡∏ó‡πà‡∏á‡∏Å‡∏£‡∏≤‡∏ü
        for bar, value in zip(bars, beyond_values):
            axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{value:.3f}', ha='center', va='bottom')
        
        # 4. Metrics Comparison
        all_metrics = ['Precision@10', 'Recall@10', 'F1@10', 'Hit Rate@10', 'NDCG@10', 'MRR']
        all_values = [
            self.evaluation_results['precision@10'],
            self.evaluation_results['recall@10'],
            self.evaluation_results['f1@10'],
            self.evaluation_results['hit_rate@10'],
            self.evaluation_results['ndcg@10'],
            self.evaluation_results['mrr']
        ]
        
        bars = axes[1, 1].bar(all_metrics, all_values, color='steelblue')
        axes[1, 1].set_ylabel('Score')
        axes[1, 1].set_title('üìä Key Metrics Comparison', fontweight='bold')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏ö‡∏ô‡πÅ‡∏ó‡πà‡∏á‡∏Å‡∏£‡∏≤‡∏ü
        for bar, value in zip(bars, all_values):
            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{value:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
    
    def compare_models(self, models_dict, user_item_matrix, df_menu, df_orders):
        """
        ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß
        
        Parameters:
        models_dict: dictionary ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ {'model_name': model_instance}
        """
        comparison_results = {}
        
        print("üîç ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ...")
        
        for model_name, model in models_dict.items():
            print(f"\nüìä ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô {model_name}...")
            evaluator = AdvancedRecommendationEvaluator(model)
            results = evaluator.evaluate_comprehensive(user_item_matrix, df_menu, df_orders)
            comparison_results[model_name] = results
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á comparison
        comparison_df = pd.DataFrame(comparison_results).T
        
        print("\n" + "="*80)
        print("üìä ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•")
        print("="*80)
        print(comparison_df.round(4))
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
        self._plot_model_comparison(comparison_df)
        
        return comparison_df
    
    def _plot_model_comparison(self, comparison_df):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. Precision@K comparison
        precision_cols = [col for col in comparison_df.columns if 'precision@' in col]
        comparison_df[precision_cols].plot(kind='bar', ax=axes[0, 0])
        axes[0, 0].set_title('üìà Precision@K Comparison', fontweight='bold')
        axes[0, 0].set_ylabel('Precision')
        axes[0, 0].legend()
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. NDCG@K comparison
        ndcg_cols = [col for col in comparison_df.columns if 'ndcg@' in col]
        comparison_df[ndcg_cols].plot(kind='bar', ax=axes[0, 1])
        axes[0, 1].set_title('üéØ NDCG@K Comparison', fontweight='bold')
        axes[0, 1].set_ylabel('NDCG')
        axes[0, 1].legend()
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. Beyond Accuracy comparison
        beyond_cols = ['mrr', 'diversity', 'novelty', 'catalog_coverage']
        comparison_df[beyond_cols].plot(kind='bar', ax=axes[1, 0])
        axes[1, 0].set_title('üåà Beyond Accuracy Comparison', fontweight='bold')
        axes[1, 0].set_ylabel('Score')
        axes[1, 0].legend()
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. Overall performance radar chart
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≤‡∏ü‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á metrics ‡∏´‡∏•‡∏±‡∏Å‡πÜ
        key_metrics = ['precision@10', 'recall@10', 'ndcg@10', 'mrr', 'diversity']
        if len(key_metrics) <= len(comparison_df.columns):
            avg_scores = comparison_df[key_metrics].mean(axis=1)
            avg_scores.plot(kind='bar', ax=axes[1, 1], color='steelblue')
            axes[1, 1].set_title('üèÜ Overall Performance', fontweight='bold')
            axes[1, 1].set_ylabel('Average Score')
            axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()


class SimpleEvaluator:
    """‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ - backward compatibility"""
    
    def __init__(self, model):
        self.model = model
        self.advanced_evaluator = AdvancedRecommendationEvaluator(model)
    
    def evaluate_model(self, user_item_matrix, df_menu, df_orders):
        """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢"""
        return self.advanced_evaluator.evaluate_comprehensive(
            user_item_matrix, df_menu, df_orders
        )
    
    def print_results(self):
        """‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"""
        self.advanced_evaluator.print_evaluation_results()
    
    def plot_results(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•"""
        self.advanced_evaluator.plot_evaluation_metrics()
